{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "#load dataset\n",
        "file_path = \"original_preprocess.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "df.drop(columns=[\"date\", \"failed_experiment\", \"cpu_platform\", \"UL/DL\", \"rapl_var\", \"n_pm\", \"pm_median\", \"txgain_ul\", \"clockspeed\", \"cqi_dl\", \"bsr_ul\", \"num_ues\", \"bler_dl\"], inplace=True, errors='ignore')\n",
        "\n",
        "features_to_analyze = [\"txgain_ul\", \"selected_mcs_ul\", \"selected_mcs_dl\", \"airtime_ul\", \"airtime_dl\"]\n",
        "target_variable = \"pm_power\"\n",
        "\n",
        "results = {\n",
        "    feature: {\"value\": [], \"mae\": [], \"mse\": [], \"rmse\": [], \"mape\": [],\n",
        "              \"relative_errors\": [], \"mean_actual\": [], \"mean_predicted\": [],\n",
        "              \"std_relative_errors\": [], \"percentiles\": {}}\n",
        "    for feature in features_to_analyze\n",
        "}\n",
        "\n",
        "#loop through each feature\n",
        "for feature in features_to_analyze:\n",
        "    print(f\"Processing feature: {feature}\")\n",
        "    unique_values = sorted(df[feature].unique())\n",
        "\n",
        "    for value in unique_values:\n",
        "        print(f\"Processing value: {value}\")\n",
        "\n",
        "        df_value = df[df[feature] == value]\n",
        "\n",
        "        if len(df_value) < 50:\n",
        "            print(f\"Skipping value {value} due to not enough data.\")\n",
        "            continue\n",
        "\n",
        "        #separate features and target\n",
        "        X = df_value.drop(columns=[target_variable] + features_to_analyze)\n",
        "        y = df_value[target_variable]\n",
        "        X = X.select_dtypes(include=[np.number])\n",
        "        X_std = X.std().replace(0, 1)\n",
        "        X = (X - X.mean()) / X_std\n",
        "\n",
        "        #80-20 train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)\n",
        "\n",
        "        #Baseline DNN model\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(32, activation='relu'),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "        model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=16, verbose=0)\n",
        "\n",
        "        #predict\n",
        "        y_pred = model.predict(X_test).flatten()\n",
        "        if np.isnan(y_pred).any():\n",
        "            continue\n",
        "\n",
        "        #calculate metrics\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "        #relative error for each test point\n",
        "        relative_errors = np.abs((y_test - y_pred) / y_test) * 100\n",
        "        mean_actual = np.mean(y_test)\n",
        "        mean_predicted = np.mean(y_pred)\n",
        "        std_relative_errors = np.std(relative_errors)\n",
        "\n",
        "        #store results\n",
        "        results[feature][\"value\"].append(value)\n",
        "        results[feature][\"mae\"].append(mae)\n",
        "        results[feature][\"mse\"].append(mse)\n",
        "        results[feature][\"rmse\"].append(rmse)\n",
        "        results[feature][\"mape\"].append(mape)\n",
        "        results[feature][\"relative_errors\"].append(np.mean(relative_errors))\n",
        "        results[feature][\"std_relative_errors\"].append(std_relative_errors)\n",
        "        results[feature][\"mean_actual\"].append(mean_actual)\n",
        "        results[feature][\"mean_predicted\"].append(mean_predicted)\n",
        "        results[feature][\"percentiles\"][value] = {\n",
        "            \"25th Percentile\": np.percentile(relative_errors, 25),\n",
        "            \"50th Percentile (Median)\": np.percentile(relative_errors, 50),\n",
        "            \"75th Percentile\": np.percentile(relative_errors, 75),\n",
        "        }\n",
        "\n",
        "#mean Relative Error % with standard deviation error bars\n",
        "for feature in features_to_analyze:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    values = results[feature][\"value\"]\n",
        "    mean_relative_errors = results[feature][\"relative_errors\"]\n",
        "    std_relative_errors = results[feature][\"std_relative_errors\"]\n",
        "    plt.errorbar(values, mean_relative_errors, yerr=std_relative_errors, fmt='o-', capsize=5, label='Relative Error (%)', color='blue')\n",
        "    plt.xlabel(\"Feature value\")\n",
        "    plt.ylabel(\"Mean Relative Error (%)\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "#mean Actual against mean predicted power with std error bars\n",
        "for feature in features_to_analyze:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    values = results[feature][\"value\"]\n",
        "    mean_actual = results[feature][\"mean_actual\"]\n",
        "    mean_predicted = results[feature][\"mean_predicted\"]\n",
        "    std_predicted = results[feature][\"std_relative_errors\"]\n",
        "    plt.errorbar(values, mean_actual, yerr=std_predicted, fmt='o-', capsize=5, label='Mean Actual Power (W)', color='red')\n",
        "    plt.errorbar(values, mean_predicted, yerr=std_predicted, fmt='s-', capsize=5, label='Mean Predicted Power (W)', color='green')\n",
        "    plt.xlabel(\"Feature value\")\n",
        "    plt.ylabel(\"Power (W)\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "#percentile values of relative errors\n",
        "for feature in features_to_analyze:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    values = results[feature][\"value\"]\n",
        "    percentile_data = [results[feature][\"percentiles\"][value] for value in values]\n",
        "\n",
        "    #percentiles\n",
        "    p25 = [p[\"25th Percentile\"] for p in percentile_data]\n",
        "    p50 = [p[\"50th Percentile (Median)\"] for p in percentile_data]\n",
        "    p75 = [p[\"75th Percentile\"] for p in percentile_data]\n",
        "    plt.plot(values, p25, 'b--', label='25th Percentile')\n",
        "    plt.plot(values, p50, 'g-', label='50th Percentile (Median)')\n",
        "    plt.plot(values, p75, 'r--', label='75th Percentile')\n",
        "    plt.xlabel(\"Feature value\")\n",
        "    plt.ylabel(\"Mean Relative Error (%)\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "for feature in features_to_analyze:\n",
        "    print(f\"\\nResults for {feature}:\")\n",
        "    result_df = pd.DataFrame({\n",
        "        \"value\": results[feature][\"value\"],\n",
        "        \"MAE (W)\": results[feature][\"mae\"],\n",
        "        \"MSE (W^2)\": results[feature][\"mse\"],\n",
        "        \"RMSE (W)\": results[feature][\"rmse\"],\n",
        "        \"MAPE (%)\": results[feature][\"mape\"]\n",
        "    })\n",
        "    print(result_df)\n",
        "\n",
        "    print(\"\\nPercentile Errors:\")\n",
        "    percentiles_df = pd.DataFrame(results[feature][\"percentiles\"]).T\n",
        "    percentiles_df.index.name = \"Feature value\"\n",
        "    print(percentiles_df)"
      ],
      "metadata": {
        "id": "rGhUcqXLiUhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igOwBTc7eKrP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file_path = \"original_preprocess.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "df.drop(columns=[\"date\", \"failed_experiment\", \"cpu_platform\", \"UL/DL\", \"rapl_var\", \"n_pm\", \"pm_median\", \"txgain_ul\", \"clockspeed\", \"cqi_dl\", \"bsr_ul\", \"num_ues\", \"bler_dl\"], inplace=True, errors='ignore')\n",
        "\n",
        "features_to_analyze = [\"txgain_ul\", \"selected_mcs_ul\", \"selected_mcs_dl\", \"airtime_ul\", \"airtime_dl\"]\n",
        "target_variable = \"pm_power\"\n",
        "\n",
        "results = {\n",
        "    feature: {\"value\": [], \"mae\": [], \"mse\": [], \"rmse\": [], \"mape\": [],\n",
        "              \"relative_errors\": [], \"mean_actual\": [], \"mean_predicted\": [],\n",
        "              \"std_relative_errors\": [], \"percentiles\": {}}\n",
        "    for feature in features_to_analyze\n",
        "}\n",
        "\n",
        "#advanced regularised DNN model\n",
        "def build_advanced_model(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Input(shape=(input_shape,)),\n",
        "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "for feature in features_to_analyze:\n",
        "    print(f\"Processing feature: {feature}\")\n",
        "    unique_values = sorted(df[feature].unique())\n",
        "\n",
        "    for value in unique_values:\n",
        "        print(f\"Processing value: {value}\")\n",
        "        df_value = df[df[feature] == value]\n",
        "\n",
        "        if len(df_value) < 50:\n",
        "            print(f\"Skipping value {value} due to not enough data.\")\n",
        "            continue\n",
        "\n",
        "        X = df_value.drop(columns=[target_variable] + features_to_analyze)\n",
        "        y = df_value[target_variable]\n",
        "        X = X.select_dtypes(include=[np.number])\n",
        "        scaler = StandardScaler()\n",
        "        X = scaler.fit_transform(X)\n",
        "\n",
        "        #80-20 train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)\n",
        "        model = build_advanced_model(X_train.shape[1])\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "        #early stopping callback\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss', patience=10, restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_test, y_test),\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=0\n",
        "        )\n",
        "        y_pred = model.predict(X_test).flatten()\n",
        "\n",
        "        if np.isnan(y_pred).any():\n",
        "            continue\n",
        "\n",
        "        #calculate metrics\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "        relative_errors = np.abs((y_test - y_pred) / y_test) * 100\n",
        "        mean_actual = np.mean(y_test)\n",
        "        mean_predicted = np.mean(y_pred)\n",
        "        std_relative_errors = np.std(relative_errors)\n",
        "\n",
        "        #store results\n",
        "        results[feature][\"value\"].append(value)\n",
        "        results[feature][\"mae\"].append(mae)\n",
        "        results[feature][\"mse\"].append(mse)\n",
        "        results[feature][\"rmse\"].append(rmse)\n",
        "        results[feature][\"mape\"].append(mape)\n",
        "        results[feature][\"relative_errors\"].append(np.mean(relative_errors))\n",
        "        results[feature][\"std_relative_errors\"].append(std_relative_errors)\n",
        "        results[feature][\"mean_actual\"].append(mean_actual)\n",
        "        results[feature][\"mean_predicted\"].append(mean_predicted)\n",
        "        results[feature][\"percentiles\"][value] = {\n",
        "            \"10th Percentile\": np.percentile(relative_errors, 10),\n",
        "            \"25th Percentile\": np.percentile(relative_errors, 25),\n",
        "            \"50th Percentile (Median)\": np.percentile(relative_errors, 50),\n",
        "            \"75th Percentile\": np.percentile(relative_errors, 75),\n",
        "            \"90th Percentile\": np.percentile(relative_errors, 90),\n",
        "        }\n",
        "\n",
        "for feature in features_to_analyze:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    values = results[feature][\"value\"]\n",
        "    mean_relative_errors = results[feature][\"relative_errors\"]\n",
        "    std_relative_errors = results[feature][\"std_relative_errors\"]\n",
        "\n",
        "    plt.errorbar(values, mean_relative_errors, yerr=std_relative_errors, fmt='o-', capsize=5, label='Relative Error (%)', color='blue')\n",
        "    plt.xlabel(\"Parameter groups\")\n",
        "    plt.ylabel(\"Mean Relative Error (%)\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "for feature in features_to_analyze:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    values = results[feature][\"value\"]\n",
        "    mean_actual = results[feature][\"mean_actual\"]\n",
        "    mean_predicted = results[feature][\"mean_predicted\"]\n",
        "    std_predicted = results[feature][\"std_relative_errors\"]\n",
        "\n",
        "    plt.errorbar(values, mean_actual, yerr=std_predicted, fmt='o-', capsize=5, label='Actual Power (W)', color='red')\n",
        "    plt.errorbar(values, mean_predicted, yerr=std_predicted, fmt='s-', capsize=5, label='Predicted Power (W)', color='green')\n",
        "    plt.xlabel(\"Parameter groups\")\n",
        "    plt.ylabel(\"Power (W)\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "for feature in features_to_analyze:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    values = results[feature][\"value\"]\n",
        "    percentile_data = [results[feature][\"percentiles\"][value] for value in values]\n",
        "\n",
        "    p10 = [p[\"10th Percentile\"] for p in percentile_data]\n",
        "    p25 = [p[\"25th Percentile\"] for p in percentile_data]\n",
        "    p50 = [p[\"50th Percentile (Median)\"] for p in percentile_data]\n",
        "    p75 = [p[\"75th Percentile\"] for p in percentile_data]\n",
        "    p90 = [p[\"90th Percentile\"] for p in percentile_data]\n",
        "\n",
        "    #percentiles\n",
        "    plt.plot(values, p10, 'm--', label='10th Percentile')\n",
        "    plt.plot(values, p25, 'b--', label='25th Percentile')\n",
        "    plt.plot(values, p50, 'g-', label='50th Percentile (Median)')\n",
        "    plt.plot(values, p75, 'r--', label='75th Percentile')\n",
        "    plt.plot(values, p90, 'c--', label='90th Percentile')\n",
        "    plt.xlabel(\"Parameter groups\")\n",
        "    plt.ylabel(\"Relative Error (%)\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "for feature in features_to_analyze:\n",
        "    print(f\"\\nResults for {feature}:\")\n",
        "    result_df = pd.DataFrame({\n",
        "        \"value\": results[feature][\"value\"],\n",
        "        \"MAE (W)\": results[feature][\"mae\"],\n",
        "        \"MSE (W^2)\": results[feature][\"mse\"],\n",
        "        \"RMSE (W)\": results[feature][\"rmse\"],\n",
        "        \"MAPE (%)\": results[feature][\"mape\"]\n",
        "    })\n",
        "    print(result_df)\n",
        "\n",
        "    print(\"\\nPercentile Errors:\")\n",
        "    percentiles_df = pd.DataFrame(results[feature][\"percentiles\"]).T\n",
        "    percentiles_df.index.name = \"Feature value\"\n",
        "    print(percentiles_df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuDr2n-M2By2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBRegressor\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "SEED = 99\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "\n",
        "file_path = \"original_preprocess.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "df.drop(columns=[\"date\", \"failed_experiment\", \"cpu_platform\", \"UL/DL\", \"rapl_var\", \"n_pm\", \"pm_median\", \"txgain_ul\", \"clockspeed\", \"cqi_dl\", \"bsr_ul\", \"num_ues\", \"bler_dl\"], inplace=True, errors='ignore')\n",
        "\n",
        "\n",
        "#feature engineering\n",
        "df['txgain_airtime_dl'] = df['txgain_dl'] * df['selected_airtime_dl']\n",
        "df['txgain_airtime_ul'] = df['txgain_ul'] * df['selected_airtime_ul']\n",
        "df['mcs_dl_airtime'] = df['selected_mcs_dl'] * df['selected_airtime_dl']\n",
        "df['mcs_ul_airtime'] = df['selected_mcs_ul'] * df['selected_airtime_ul']\n",
        "df['mcs_gap'] = df['selected_mcs_dl'] - df['selected_mcs_ul']\n",
        "df['snr_per_bler'] = df['mean_snr_ul'] / (df['bler_ul'] + 1e-6)\n",
        "df['throughput_total'] = df['thr_dl'] + df['thr_ul']\n",
        "df['airtime_total'] = df['airtime_dl'] + df['airtime_ul']\n",
        "\n",
        "features_to_analyze = [\"txgain_ul\", \"selected_mcs_ul\", \"selected_mcs_dl\", \"airtime_ul\", \"airtime_dl\"]\n",
        "\n",
        "target_variable = \"pm_power\"\n",
        "\n",
        "resultsdx = {\n",
        "    feature: {\n",
        "        \"value\": [], \"mae\": [], \"mse\": [], \"rmse\": [], \"mape\": [],\n",
        "        \"relative_errors\": [], \"mean_actual\": [], \"mean_predicted\": [],\n",
        "        \"std_relative_errors\": [], \"percentiles\": {}\n",
        "    }\n",
        "    for feature in features_to_analyze\n",
        "}\n",
        "\n",
        "\n",
        "hyperparams = {\n",
        "    \"dense_units_1\": 587,\n",
        "    \"l2_1\": 0.000511,\n",
        "    \"dropout_1\": 0.39,\n",
        "    \"dense_units_2\": 261,\n",
        "    \"l2_2\": 0.00563,\n",
        "    \"dropout_2\": 0.402,\n",
        "    \"dense_units_3\": 186,\n",
        "    \"dense_units_4\": 99,\n",
        "    \"learning_rate\": 0.00237,\n",
        "    \"epochs\": 263,\n",
        "    \"batch_size\": 32,\n",
        "    \"xgb_n_estimators\": 256,\n",
        "    \"xgb_max_depth\": 5,\n",
        "    \"xgb_learning_rate\": 0.2276,\n",
        "    \"xgb_subsample\": 0.5006,\n",
        "    \"xgb_colsample_bytree\": 0.8246,\n",
        "    \"xgb_reg_lambda\": 4.5554,\n",
        "    \"xgb_reg_alpha\": 3.5152\n",
        "}\n",
        "\n",
        "\n",
        "#hybrid DNN-XGBoost model\n",
        "def build_optimized_model(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Input(shape=(input_shape,)),\n",
        "        layers.Dense(hyperparams[\"dense_units_1\"], activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(hyperparams[\"l2_1\"])),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(hyperparams[\"dropout_1\"]),\n",
        "\n",
        "        layers.Dense(hyperparams[\"dense_units_2\"], activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(hyperparams[\"l2_2\"])),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(hyperparams[\"dropout_2\"]),\n",
        "\n",
        "        layers.Dense(hyperparams[\"dense_units_3\"], activation='relu'),\n",
        "        layers.Dense(hyperparams[\"dense_units_4\"], activation='relu')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "#compile optimised DNN model\n",
        "def compile_model(model):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=hyperparams[\"learning_rate\"])\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "\n",
        "#loop through each feature\n",
        "for feature in features_to_analyze:\n",
        "    print(f\"Processing feature: {feature}\")\n",
        "    unique_values = sorted(df[feature].unique())\n",
        "    for value in unique_values:\n",
        "        print(f\"Processing value: {value}\")\n",
        "        df_value = df[df[feature] == value]\n",
        "        if len(df_value) < 50:\n",
        "            print(f\"Skipping value {value} due to not enough data.\")\n",
        "            continue\n",
        "\n",
        "        X = df_value.drop(columns=[target_variable] + features_to_analyze)\n",
        "        y = df_value[target_variable]\n",
        "        X = X.select_dtypes(include=[np.number])\n",
        "        scaler = StandardScaler()\n",
        "        X = scaler.fit_transform(X)\n",
        "\n",
        "        #80-20 train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=99 )\n",
        "        dnn_model = build_optimized_model(X_train.shape[1])\n",
        "        dnn_model = compile_model(dnn_model)\n",
        "\n",
        "        #train the DNN model\n",
        "        dnn_model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_test, y_test),\n",
        "            epochs=hyperparams[\"epochs\"],\n",
        "            batch_size=hyperparams[\"batch_size\"],\n",
        "            verbose=0\n",
        "        )\n",
        "        X_train_features = dnn_model.predict(X_train)\n",
        "        X_test_features = dnn_model.predict(X_test)\n",
        "\n",
        "        #train XGBoost on the extracted features\n",
        "        xgb_model = XGBRegressor(\n",
        "            objective='reg:squarederror',\n",
        "            n_estimators=hyperparams[\"xgb_n_estimators\"],\n",
        "            learning_rate=hyperparams[\"xgb_learning_rate\"],\n",
        "            max_depth=hyperparams[\"xgb_max_depth\"],\n",
        "            subsample=hyperparams[\"xgb_subsample\"],\n",
        "            colsample_bytree=hyperparams[\"xgb_colsample_bytree\"],\n",
        "            reg_lambda=hyperparams[\"xgb_reg_lambda\"],\n",
        "            reg_alpha=hyperparams[\"xgb_reg_alpha\"],\n",
        "            random_state=99\n",
        "        )\n",
        "        xgb_model.fit(X_train_features, y_train)\n",
        "        y_pred = xgb_model.predict(X_test_features)\n",
        "\n",
        "        if np.isnan(y_pred).any():\n",
        "            continue\n",
        "\n",
        "        #calculate metrics\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "        relative_errors = np.abs((y_test - y_pred) / y_test) * 100\n",
        "\n",
        "        #results\n",
        "        resultsdx[feature][\"value\"].append(value)\n",
        "        resultsdx[feature][\"mae\"].append(mae)\n",
        "        resultsdx[feature][\"mse\"].append(mse)\n",
        "        resultsdx[feature][\"rmse\"].append(rmse)\n",
        "        resultsdx[feature][\"mape\"].append(mape)\n",
        "        resultsdx[feature][\"relative_errors\"].append(np.mean(relative_errors))\n",
        "        resultsdx[feature][\"mean_actual\"].append(np.mean(y_test))\n",
        "        resultsdx[feature][\"mean_predicted\"].append(np.mean(y_pred))\n",
        "        resultsdx[feature][\"std_relative_errors\"].append(np.std(relative_errors))\n",
        "        resultsdx[feature][\"percentiles\"][value] = {\n",
        "            \"10th Percentile\": np.percentile(relative_errors, 10),\n",
        "            \"25th Percentile\": np.percentile(relative_errors, 25),\n",
        "            \"50th Percentile (Median)\": np.percentile(relative_errors, 50),\n",
        "            \"75th Percentile\": np.percentile(relative_errors, 75),\n",
        "            \"90th Percentile\": np.percentile(relative_errors, 90),\n",
        "        }\n",
        "\n",
        "for feature in features_to_analyze:\n",
        "    print(f\"\\nResults for {feature}:\")\n",
        "    result_df = pd.DataFrame({\n",
        "        \"value\": resultsdx[feature][\"value\"],\n",
        "        \"MAE (W)\": resultsdx[feature][\"mae\"],\n",
        "        \"MSE (W^2)\": resultsdx[feature][\"mse\"],\n",
        "        \"RMSE (W)\": resultsdx[feature][\"rmse\"],\n",
        "        \"MAPE (%)\": resultsdx[feature][\"mape\"]\n",
        "    })\n",
        "    print(result_df)\n",
        "\n",
        "    print(\"\\nPercentile Errors:\")\n",
        "    percentiles_df = pd.DataFrame(resultsdx[feature][\"percentiles\"]).T\n",
        "    percentiles_df.index.name = \"Feature value\"\n",
        "    print(percentiles_df)\n",
        "\n",
        "for feature in features_to_analyze:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.errorbar(resultsdx[feature][\"value\"], resultsdx[feature][\"relative_errors\"],\n",
        "                 yerr=resultsdx[feature][\"std_relative_errors\"], fmt='o-', capsize=5)\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel(\"Mean Relative Error (%)\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "for feature in features_to_analyze:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.errorbar(resultsdx[feature][\"value\"], resultsdx[feature][\"mean_actual\"],\n",
        "                 yerr=resultsdx[feature][\"std_relative_errors\"], fmt='o-', capsize=5, label='Actual Power', color='red')\n",
        "    plt.errorbar(resultsdx[feature][\"value\"], resultsdx[feature][\"mean_predicted\"],\n",
        "                 yerr=resultsdx[feature][\"std_relative_errors\"], fmt='s-', capsize=5, label='Predicted Power', color='green')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel(\"Power (W)\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"original_preprocess.csv\")\n",
        "\n",
        "\n",
        "df[\"tx_ul\"] = df[\"txgain_ul\"] * df[\"airtime_ul\"]\n",
        "df[\"tx_dl\"] = df[\"txgain_dl\"] * df[\"airtime_dl\"]\n",
        "\n",
        "#analytical model - Linear gain-airtime\n",
        "def model_gain(tx_ul, tx_dl):\n",
        "    alpha, beta, gamma = 0.03, 0.01, 12\n",
        "    return alpha * tx_ul + beta * tx_dl + gamma\n",
        "\n",
        "#analytical model 2 - Airtime-based quadratic\n",
        "def model_airtime(airtime_ul, airtime_dl):\n",
        "    P_idle = 6\n",
        "    b1_ul, b2_ul = 4, 5\n",
        "    b1_dl, b2_dl = 3, 4\n",
        "    return (P_idle +\n",
        "            b1_ul * airtime_ul + b2_ul * airtime_ul**2 +\n",
        "            b1_dl * airtime_dl + b2_dl * airtime_dl**2)\n",
        "\n",
        "\n",
        "df[\"pred_power_gain\"] = model_gain(df[\"tx_ul\"], df[\"tx_dl\"])\n",
        "df[\"pred_power_airtime\"] = model_airtime(df[\"airtime_ul\"], df[\"airtime_dl\"])\n",
        "\n",
        "group_features = [\"txgain_ul\", \"selected_mcs_ul\", \"selected_mcs_dl\", \"airtime_ul\", \"airtime_dl\"]\n",
        "\n",
        "model_info = {\n",
        "    \"Gain Model\": \"pred_power_gain\",\n",
        "    \"Airtime Model\": \"pred_power_airtime\"\n",
        "}\n",
        "\n",
        "for model_name, pred_col in model_info.items():\n",
        "    print(f\"\\ Evaluation for: {model_name}\")\n",
        "\n",
        "    for feature in group_features:\n",
        "        print(f\"\\n Grouped by '{feature}'\")\n",
        "        summary = []\n",
        "\n",
        "        for val, group in df.groupby(feature):\n",
        "            if len(group) < 50:\n",
        "                continue\n",
        "\n",
        "            y_true = group[\"pm_power\"]\n",
        "            y_pred = group[pred_col]\n",
        "            mre = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "            summary.append({\n",
        "                feature: val,\n",
        "                \"Mean Relative Error (%)\": mre\n",
        "            })\n",
        "        plot_df = pd.DataFrame(summary)\n",
        "        print(plot_df.to_string(index=False))\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.title(f\"{model_name} - MRE (%) grouped by {feature}\")\n",
        "        plt.xlabel(feature)\n",
        "        plt.ylabel(\"Mean Relative Error (%)\")\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "XeOFFWiE0Gru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDiaOP46LCn7"
      },
      "outputs": [],
      "source": [
        "file_path = 'original_preprocess.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "#config params\n",
        "config_features = [\n",
        "    'txgain_ul',\n",
        "    'selected_mcs_dl',\n",
        "    'selected_mcs_ul',\n",
        "    'selected_airtime_dl',\n",
        "    'selected_airtime_ul',\n",
        "]\n",
        "\n",
        "group_counts = df.groupby(config_features).size().reset_index(name='count')\n",
        "repeated_configs = group_counts[group_counts['count'] >= 2]\n",
        "df_repeats = df.merge(repeated_configs[config_features], on=config_features, how='inner')\n",
        "std_devs = df_repeats.groupby(config_features)['pm_power'].std().dropna()\n",
        "intrinsic_noise = std_devs.mean()\n",
        "mean_pm_power = df['pm_power'].mean()\n",
        "\n",
        "\n",
        "intrinsic_noise_percent = (intrinsic_noise / mean_pm_power) * 100\n",
        "print(f\"Estimated intrinsic noise (std dev of power consumption): {intrinsic_noise:.4f} W\")\n",
        "print(f\"Intrinsic noise as percentage of mean power consumption: {intrinsic_noise_percent:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}